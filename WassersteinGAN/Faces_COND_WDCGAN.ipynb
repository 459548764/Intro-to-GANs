{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faces_COND_WDCGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Machine-Learning-Tokyo/Intro-to-GANs/blob/master/WassersteinGAN/Faces_COND_WDCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "u_u_LBqlTI0B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Faces Wasserstein GAN (WGAN)\n",
        "\n",
        "In this notebook we use the same model in [Wasserstein GAN (WGAN)  -- Solutions]() but used to train on a dataset of faces. As a result, our GAN would produce faces of people.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "2zmKlEq3WvQ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Donwload dataset\n",
        "\n",
        "First of all we need a dataset of faces. We're going to use the [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset, a dataset with over 200k pictures of celebrities.\n",
        "\n",
        "To download it we're going to use an script that belongs to the [StarGAN](https://github.com/yunjey/stargan) project. StarGAN is an advanced GAN that modifies faces of people. There is no need to understand it for this notebook, but go ahead and have a look if you have curiosity.\n",
        "\n",
        "So run the following cells to download the dataset. This will take a while."
      ]
    },
    {
      "metadata": {
        "id": "fmFXlnvpKo2S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#%%capture\n",
        "# download CelebA data\n",
        "!wget https://raw.githubusercontent.com/yunjey/StarGAN/master/download.sh\n",
        "!bash download.sh celeba"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XEOZ22-rb1VP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!echo \"There are `ls data/celeba/images | wc -l` images\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KlRx5ymdZG0E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "tHZUZmOUNEoJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, BatchNormalization, Reshape, Flatten\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "\n",
        "from keras.layers import Conv2D, UpSampling2D, concatenate, Lambda\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from PIL import Image\n",
        "from matplotlib import animation, rc\n",
        "from IPython.display import Image as ipyImage\n",
        "from IPython.display import HTML\n",
        "from os import listdir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F-JAtyy4WlDS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hidden data loader code\n",
        "\n",
        "To use a dataset we need a data loader. In our previous WGAN notebooks we used a pre-loaded dataset: fashionMNIST. fashionMNIST is a very easy and rather small dataset so images ar provided inside an array that can be indexed to generate batches. On the contrary, CelebA is a very big one, so it's not feasible to keep it in memory inside an arry. What we're going to do is to implement a loader that will reach the image files in disc and generate the batches on the fly.\n",
        "\n",
        "The code for this data loader is hidden.  It's cumbersome and it's not the point of this exercise so you can ignore it."
      ]
    },
    {
      "metadata": {
        "id": "Nem1E1xptitC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# templates\n",
        "DATA_DIR = 'data/celeba/'\n",
        "IMGS_DIR = '{}images/'.format(DATA_DIR)\n",
        "CSV_FILE = '{}list_attr_celeba.txt'.format(DATA_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VhXZuBevWkdD",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Data loader\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from itertools import cycle\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from random import shuffle\n",
        "import random\n",
        "\n",
        "def download_celeba(data_dir, imgs_dir):\n",
        "  %rm -r {data_dir}\n",
        "  %mkdir {data_dir}\n",
        "  %cd {data_dir}\n",
        "\n",
        "  !pip install pydrive\n",
        "  from shutil import unpack_archive\n",
        "  # these classes allow you to request the Google drive API\n",
        "  from pydrive.auth import GoogleAuth\n",
        "  from pydrive.drive import GoogleDrive\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "\n",
        "  from googleapiclient.http import MediaIoBaseDownload\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  file_id_dict = {\n",
        "      'imgs.zip': '0B7EVK8r0v71pZjFTYXZWM3FlRnM',\n",
        "      'attributes.txt': '0B7EVK8r0v71pblRyaVFSWGxPY0U'\n",
        "  }\n",
        "  for file_, id_ in file_id_dict.items():\n",
        "    downloaded = drive.CreateFile({'id': id_})\n",
        "    downloaded.GetContentFile(file_)\n",
        "\n",
        "#   %rm -r {imgs_dir}*\n",
        "  unpack_archive('imgs.zip', imgs_dir)\n",
        "  %rm imgs.zip\n",
        "\n",
        "def get_imgs_lists(df, category):\n",
        "  pos_img_names = df.index[df[category] == 1]\n",
        "  neg_img_names = df.index[df[category] == -1]\n",
        "  \n",
        "  return list(pos_img_names), list(neg_img_names)\n",
        "\n",
        "def preprocess_img(img, img_shape, crop=None):\n",
        "  img = img / 127.5 - 1\n",
        "  if crop is not None:\n",
        "    cropx, cropy = crop\n",
        "    y,x,_ = img.shape\n",
        "    startx = x//2-(cropx//2)\n",
        "    starty = y//2-(cropy//2)\n",
        "    img = img[starty:starty+cropy,startx:startx+cropx,:]\n",
        "  img = cv2.resize(img, img_shape[:2])\n",
        "  \n",
        "  return img\n",
        "\n",
        "def single_core_category_generator(img_paths, img_shape):\n",
        "  imgs = np.zeros((len(img_paths), *img_shape))\n",
        "  for i, img_path in enumerate(img_paths):\n",
        "    img = mpimg.imread(img_path)\n",
        "    imgs[i] = preprocess_img(img, img_shape)\n",
        "  return imgs\n",
        "\n",
        "\n",
        "def mp_category_generator(imgs_dir, img_names, img_shape, batch_size):\n",
        "  import multiprocessing as mp\n",
        "  cpu_num = min(mp.cpu_count(), batch_size)\n",
        "  \n",
        "  def chunks(lst, n):\n",
        "    for i in range(n):\n",
        "        yield lst[i::n]\n",
        "  \n",
        "  shuffle(img_names)\n",
        "  img_names = cycle(img_names)\n",
        "  \n",
        "  while True:\n",
        "    imgs = np.zeros((batch_size, *img_shape))\n",
        "    batch_paths = [imgs_dir + next(img_names) for _ in range(batch_size)]\n",
        "    batch_paths = chunks(batch_paths, cpu_num)\n",
        "    \n",
        "    pool = mp.Pool(processes=cpu_num)\n",
        "    imgs = [pool.apply(single_core_category_generator,\n",
        "                       args=(next(batch_paths), img_shape)) for i in range(cpu_num)]\n",
        "    pool.terminate()\n",
        "    imgs = np.concatenate(imgs)\n",
        "    \n",
        "    yield imgs\n",
        "    \n",
        "def category_generator(imgs_dir, img_names, img_shape, batch_size, crop=None):  \n",
        "  shuffle(img_names)\n",
        "  img_names = cycle(img_names)\n",
        "  \n",
        "  while True:\n",
        "    imgs = np.zeros((batch_size, *img_shape))\n",
        "    for i in range(batch_size):\n",
        "      img_path = imgs_dir + next(img_names)\n",
        "      img = mpimg.imread(img_path)\n",
        "      imgs[i] = preprocess_img(img, img_shape, crop=crop)\n",
        "    \n",
        "    yield imgs\n",
        " \n",
        "def get_generators(img_shape, batch_size, category=None, download=True, crop=None):\n",
        "  data_dir = 'data/'\n",
        "  imgs_dir = '{}celeba/images/'.format(data_dir)\n",
        "\n",
        "  if download:\n",
        "    download_celeba(data_dir, imgs_dir)\n",
        "  \n",
        "  imgs_dir = imgs_dir\n",
        "  df = pd.read_csv('{}celeba/list_attr_celeba.txt'.format(data_dir), sep=' +', skiprows=[0])\n",
        "  \n",
        "  img_names = list(df.index)\n",
        "  if category is None:\n",
        "    gen = category_generator(imgs_dir, img_names, img_shape, batch_size)\n",
        "    return gen\n",
        "  \n",
        "  pos_img_names, neg_img_names = get_imgs_lists(df, category)\n",
        "\n",
        "  pos_gen = category_generator(imgs_dir, pos_img_names, img_shape, batch_size, crop=crop)\n",
        "  neg_gen = category_generator(imgs_dir, neg_img_names, img_shape, batch_size, crop=crop)\n",
        "  \n",
        "  return pos_gen, neg_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uvvPV_WIZoR2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Function to build the generator"
      ]
    },
    {
      "metadata": {
        "id": "6gqJ4ZrmNHve",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_generator(noise_size, img_shape, num_classes):\n",
        "  # block: Conv, Batch norm, Upsampling\n",
        "  k_size = 5, 5\n",
        "  k_init = RandomNormal(0, 0.01)\n",
        "  filters = 1024 #CHANGE\n",
        "  \n",
        "  noise = Input((noise_size,))\n",
        "  labels = Input((num_classes,))\n",
        "  \n",
        "  model_input = concatenate([noise, labels])\n",
        "  \n",
        "  x = Dense(4*4*filters, kernel_initializer=k_init, activation='relu')(model_input)\n",
        "  x = Reshape((4, 4, filters))(x)  # 4, 4\n",
        "  x = BatchNormalization()(x)\n",
        "  x = UpSampling2D()(x)  # 8, 8\n",
        "  \n",
        "  x = Conv2D(filters // 2, k_size, padding='same', kernel_initializer=k_init, activation='relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = UpSampling2D()(x)  # 16, 16\n",
        "  \n",
        "  x = Conv2D(filters // 4, k_size, padding='same', kernel_initializer=k_init, activation='relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = UpSampling2D()(x)  # 32, 32\n",
        "  \n",
        "  #CHANGE\n",
        "  x = Conv2D(filters // 8, k_size, padding='same', kernel_initializer=k_init, activation='relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = UpSampling2D()(x)  # 64, 64\n",
        "  \n",
        "  img = Conv2D(img_shape[-1], k_size, padding='same', kernel_initializer=k_init, activation='tanh')(x)\n",
        "  \n",
        "  generator = Model([noise, labels], img)\n",
        "  return generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ooEwRbP8eLgw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Function to build the discriminator"
      ]
    },
    {
      "metadata": {
        "id": "UysJTIlPOlZS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_discriminator(img_shape, num_classes):\n",
        "  # block: Conv, Batch norm, LeakyRelu\n",
        "  k_size = 5, 5\n",
        "  k_init = RandomNormal(0, 0.01)\n",
        "  filters = 1024 #CHANGE\n",
        "  \n",
        "  \n",
        "  img = Input(img_shape)  # 64, 64, 3\n",
        "  labels = Input((num_classes,))  # 10\n",
        "  \n",
        "  n_labels = Reshape((1, 1, -1))(labels)  # (batch_size), 1, 1, 10\n",
        "  n_labels = Lambda(lambda x: K.tile(x, [1, img_shape[0], img_shape[1], 1]))(n_labels)  # (batch_size), 64, 64, 10\n",
        "  model_input = concatenate([img, n_labels])  # 64, 64, ? (1 + 10)\n",
        "  \n",
        "  #CHANGE\n",
        "  x = Conv2D(filters // 8, k_size, strides=(2, 2), padding='same', kernel_initializer=k_init)(model_input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)  #32, 32\n",
        "  \n",
        "  x = Conv2D(filters // 4, k_size, strides=(2, 2), padding='same', kernel_initializer=k_init)(x)#CHANGE, model_input -> x\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)  #16, 16\n",
        "  \n",
        "  x = Conv2D(filters // 2, k_size, strides=(2, 2), padding='same', kernel_initializer=k_init)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)  # 8, 8\n",
        "  \n",
        "  x = Conv2D(filters, k_size, strides=(2, 2), padding='same', kernel_initializer=k_init)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)  # 4, 4\n",
        "  \n",
        "  x = Flatten()(x)\n",
        "  validity = Dense(3, activation='linear', kernel_initializer=k_init)(x) #CHANGE\n",
        "  \n",
        "  discriminator = Model([img, labels], validity)\n",
        "  return discriminator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uxDM6ldueOte",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Function to compile the models"
      ]
    },
    {
      "metadata": {
        "id": "qkvftz_HF8sw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def critic_loss(y_true, y_pred):\n",
        "  return K.mean(y_true * y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gEfTuy93PRNb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_compiled_models(generator, discriminator, noise_size, num_classes):\n",
        "  \n",
        "  optimizer = RMSprop(0.0002)\n",
        "  \n",
        "  discriminator.compile(optimizer, loss=critic_loss)\n",
        "  discriminator.trainable = False\n",
        "  \n",
        "  noise = Input((noise_size,))\n",
        "  labels = Input((num_classes,))\n",
        "  \n",
        "  img = generator([noise, labels])\n",
        "  validity = discriminator([img, labels])\n",
        "  combined = Model([noise, labels], validity)\n",
        "  \n",
        "  combined.compile(optimizer, loss=critic_loss)\n",
        "  \n",
        "  return generator, discriminator, combined"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ocu71YfmeSIH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Function to sample and save generated images"
      ]
    },
    {
      "metadata": {
        "id": "X4-ejg8UTwqV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#FIXME this is old code, unused\n",
        "def sample_imgs(generator, noise_size, step, plot_img=True, cond=False, num_classes=10):\n",
        "  np.random.seed(0)\n",
        "  \n",
        "  r, c = num_classes, 10\n",
        "  if cond:\n",
        "    noise = np.random.normal(0, 1, (c, noise_size))\n",
        "    noise = np.tile(noise, (r, 1))\n",
        "\n",
        "    sampled_labels = np.arange(r).reshape(-1, 1)\n",
        "    sampled_labels = to_categorical(sampled_labels, r)\n",
        "    sampled_labels = np.repeat(sampled_labels, c, axis=0)\n",
        "\n",
        "    imgs = generator.predict([noise, sampled_labels])\n",
        "  else:\n",
        "    noise = np.random.normal(0, 1, (r*c, noise_size))\n",
        "    imgs = generator.predict_on_batch(noise)\n",
        "  \n",
        "  imgs = imgs / 2 + 0.5\n",
        "  imgs = np.reshape(imgs, [r, c, imgs.shape[1], imgs.shape[2], -1])\n",
        "  \n",
        "  figsize = 1 * c, 1 * r\n",
        "  fig, axs = plt.subplots(r, c, figsize=figsize)\n",
        "  \n",
        "  for i in range(r):\n",
        "    for j in range(c):\n",
        "      img = imgs[i, j] if len(imgs.shape) == 4 else imgs[i, j, :, :, 0]\n",
        "      axs[i, j].imshow(img, cmap='gray')\n",
        "      axs[i, j].axis('off')\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  fig.savefig(f'/content/images/{step}.png')\n",
        "  if plot_img:\n",
        "    plt.show()\n",
        "  plt.close()\n",
        "  \n",
        "  np.random.seed(None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a4xsTrQsagw-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rows, cols = 4, 7\n",
        "sampled_labels = np.array([0.0 if i >= rows*cols/2 else 1.0 for i in range(rows*cols)])\n",
        "\n",
        "def sample_imgs_(generator, g_loss_buffer, noise_size, step):\n",
        "  test_images = generator.predict([test_noise, sampled_labels])\n",
        "  fig = plt.figure(1, figsize=(2*1.2*cols, 1.2*rows))\n",
        "  gs = gridspec.GridSpec(rows, 2*cols)\n",
        "  for j in range(rows*cols):\n",
        "    plt.subplot(gs[j//cols, j%cols])#invert!\n",
        "    plt.imshow(test_images[j-1]/2.0 + 0.5)\n",
        "    axs = plt.gca()\n",
        "    if j >= rows*cols/2:\n",
        "      axs.tick_params(axis=u'both', which=u'both',length=5)\n",
        "      axs.set_xticks([])\n",
        "      axs.set_yticks([])\n",
        "    else:\n",
        "      axs.axis('off')\n",
        "  #plot error here\n",
        "  plt.subplot(gs[:,cols+1:])\n",
        "  plt.plot(g_loss_buffer)\n",
        "  plt.grid(True)\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  fig.savefig('/content/images/{}.png'.format(step))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K3ofIxpleVoc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Function to train the models"
      ]
    },
    {
      "metadata": {
        "id": "D1QbGP031WWg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Select the save step for debugging images"
      ]
    },
    {
      "metadata": {
        "id": "r8v--rWc1Qbq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SAVE_STEP = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wHI7xwGMQRAW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(models, data_loader, noise_size, img_shape, num_classes, batch_size, steps):\n",
        "  \n",
        "  generator, discriminator, combined = models\n",
        "  pos_loader, neg_loader = data_loader\n",
        "  #CHANGE delete fashion mnist\n",
        "  \n",
        "  g_loss_buffer = []\n",
        "  for step in range(1, steps + 1):\n",
        "    for i in range(n_critic):\n",
        "      # train discriminator\n",
        "      if (step + i) % 2 == 0:\n",
        "        real_imgs, labels = next(pos_loader), np.ones(batch_size)\n",
        "      else:\n",
        "        real_imgs, labels = next(neg_loader), np.zeros(batch_size)\n",
        "      \n",
        "      noise = np.random.normal(0, 1, (batch_size, noise_size))\n",
        "      gen_imgs = generator.predict([noise, labels])\n",
        "\n",
        "      gen_validity = np.ones(batch_size)\n",
        "      real_validity = - np.ones(batch_size)\n",
        "\n",
        "      r_loss = discriminator.train_on_batch([real_imgs, labels], real_validity)\n",
        "      g_loss = discriminator.train_on_batch([gen_imgs, labels], gen_validity)\n",
        "      disc_loss = np.add(r_loss, g_loss) / 2\n",
        "        \n",
        "    # clipping\n",
        "    for layer in discriminator.layers:\n",
        "      weights = layer.get_weights()\n",
        "      clipped_weights = [np.clip(w, -c, c) for w in weights]\n",
        "      layer.set_weights(clipped_weights)\n",
        "      \n",
        "    # train generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, noise_size))\n",
        "    gen_loss = combined.train_on_batch([noise, labels], -np.ones(batch_size))\n",
        "    g_loss_buffer.append(gen_loss)\n",
        "    \n",
        "    #print progress\n",
        "    if step % SAVE_STEP == 0:\n",
        "      print('step: %d, D_loss: %f, G_loss: %f' % (step, disc_loss, gen_loss))\n",
        "    \n",
        "    # save_samples\n",
        "    if step % SAVE_STEP == 0:\n",
        "      sample_imgs_(generator, g_loss_buffer, noise_size, step)\n",
        "      \n",
        "    # save model\n",
        "    if step % 1000 == 0:\n",
        "      generator.save('faces_g_step{}.h5'.format(step))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jHnx_qUceZyc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "1-ePrmDUVBYM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%rm -r /content/images\n",
        "%mkdir /content/images\n",
        "noise_size = 100\n",
        "img_shape = 64, 64, 3 #CHANGE\n",
        "num_classes = 1 #CHANGE\n",
        "batch_size = 32\n",
        "steps = 100000\n",
        "\n",
        "c = 0.01\n",
        "n_critic = 5\n",
        "\n",
        "#CHANGE\n",
        "category = 'Male'\n",
        "data_loader = get_generators(img_shape, batch_size, category, download=False, crop=(150, 150))\n",
        "test_noise = np.random.normal(size=(rows*cols, noise_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lhkkM5ctecbb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate the models"
      ]
    },
    {
      "metadata": {
        "id": "FCr5cDHOZCdW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator = build_generator(noise_size, img_shape, num_classes)\n",
        "discriminator = build_discriminator(img_shape, num_classes)\n",
        "compiled_models = get_compiled_models(generator, discriminator, noise_size, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zTkuRejSefba",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the models"
      ]
    },
    {
      "metadata": {
        "id": "JznD-Lt6Y2h_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train(compiled_models, data_loader, noise_size, img_shape, num_classes, batch_size, steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bzPwRKZzL4bV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Plot resutls"
      ]
    },
    {
      "metadata": {
        "id": "7BDEKYo9ehjq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Display samples\n",
        "\n",
        "Let's start by checking the images that we have stored."
      ]
    },
    {
      "metadata": {
        "id": "TAkraVLWiohI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%ls /content/images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KfNsB1NO2tQM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can check any image you wish by doing:"
      ]
    },
    {
      "metadata": {
        "id": "Z0JNe-A02wVK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_number = SAVE_STEP\n",
        "ipyImage('/content/images/%d.png' % image_number)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4eGqyamNc69P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Do an animation\n",
        "\n",
        "Probably the best way of showing the training process is by doing an animation with all the images. The next cell will do it for you."
      ]
    },
    {
      "metadata": {
        "id": "S4vf2xaU8srd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = '/content/images/{}.png'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W8Ma5UFS8pux",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AnimObject(object):\n",
        "    def __init__(self, images):\n",
        "        print(len(images))\n",
        "        self.fig, self.ax = plt.subplots()\n",
        "        self.ax.set_title(\"\")\n",
        "        self.fig.set_size_inches((20, 10))\n",
        "        self.plot = plt.imshow(images[0])\n",
        "        plt.tight_layout()\n",
        "        self.images = images\n",
        "        \n",
        "    def init(self):\n",
        "        self.plot.set_data(self.images[0])\n",
        "        self.ax.grid(False)\n",
        "        return (self.plot,)\n",
        "      \n",
        "    def animate(self, i):\n",
        "        self.plot.set_data(self.images[i])\n",
        "        self.ax.grid(False)\n",
        "        self.ax.set_xticks([])\n",
        "        self.ax.set_yticks([])\n",
        "        self.ax.set_title(\"index {}\".format(i))\n",
        "        return (self.plot,)\n",
        "\n",
        "def get_figures(template, indices):\n",
        "    import os.path\n",
        "    images = []\n",
        "    for index in indices:\n",
        "        if os.path.isfile(template.format(index)):\n",
        "            images.append(Image.open(template.format(index)))\n",
        "    return images\n",
        "\n",
        "\n",
        "images = get_figures(\"/content/images/{}.png\", \n",
        "                     range(0, SAVE_STEP * len(listdir('/content/images')) + 1, SAVE_STEP))\n",
        "print(images)\n",
        "animobject = AnimObject(images)\n",
        "anim = animation.FuncAnimation(\n",
        "              animobject.fig,\n",
        "              animobject.animate,\n",
        "              frames=len(animobject.images),\n",
        "              interval=150,\n",
        "              blit=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZ7K970G-nyv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GaDzs7MYQ3ah",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download images and generator\n",
        "\n",
        "This code is to download the trained model to store it locally on your computer. You probably shouldn't bother about it."
      ]
    },
    {
      "metadata": {
        "id": "mr9q_1QsQ60o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gen_path = '/content/fashion_cond_w_dcgan_gen.h5'\n",
        "generator.save(gen_path)\n",
        "from google.colab import files\n",
        "files.download(gen_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LtK0-9JNLy8j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}