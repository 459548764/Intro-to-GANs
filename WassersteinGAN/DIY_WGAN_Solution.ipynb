{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DIY_WGAN_Solution.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Machine-Learning-Tokyo/Intro-to-GANs/blob/master/WassersteinGAN/DIY_WGAN_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "pRaS_owvsWaI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Wasserstein GAN (WGAN)  -- Solutions\n",
        "\n",
        "**This notebook is the solution to the DIY_WGAN notebook. Mind that lines that contain important changes from the previous version have been marked with a `#CHANGE` comment.**\n",
        "```python\n",
        "this_line _has_changed #CHANGE\n",
        "```\n",
        "---\n",
        "\n",
        "This notebook gives indications on how to implement a Wasserstein GAN for the fashion MNIST dataset. We take our previous Conditional DCGAN implementation as a baseline and apply the necessary changes to convert it into a Wasserstein GAN. This means that if you run this file with no modification you would be just training a Conditional DCGAN on the fashion MNIST dataset.\n",
        "\n",
        "## What is a Wasserstein GAN?\n",
        "\n",
        "The Wasserstein GAN is a type of GAN that uses the Wasserstein distance to measure the difference between the . As opposed to the Kullback-Leibler divergence, that is the one usued in the original GAN.\n",
        "\n",
        "The nuances of this new approach are discussed in the original paper: [Wasserstein GAN](https://arxiv.org/abs/1701.07875).\n",
        "\n",
        "## What should we change in the code?\n",
        "\n",
        "Although the mathematical formulation and demonstration of the Wasserstein GAN is relatively complicated, the changes needed to make an WGAN out of a normal GAN are not that many nor complicated.\n",
        "\n",
        "In total there are 4 things we need to change:\n",
        "\n",
        "+ __Activation of D__, as opposed to the original GAN, the WGAN has a linear activation, so we have to get rid of the sigmoid function at the output of D.\n",
        "+ __Loss function__. The loss function is just the difference between the output of D for real samples and the output of D for generated samples. As opposed to the original GAN, the Discriminator in WGAN does not discriminate samples as being real-looking or fake-looking, but provides a measure of how close they are to the true distribution. For this reason it is often called Critic instead of Discriminator (although in practice it is common to keep calling it D while coding). **Mind that changes in this loss function are going to need changes in how we provide the targets**.\n",
        "+ __Clipping weights and optimiser__. We need to guarantee that the function that D computes is K-Lipschitz. One way to ensure this is to clip the weights of D to a small value. This is not the only form and there are papers proposing new methods ([Improved Training of Wasserstein GANs](https://arxiv.org/pdf/1704.00028.pdf) and [Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect](https://arxiv.org/abs/1803.01541).\n",
        "+ __Training procedure: now D is trained more often__, in the WGAN it is not important to keep a balance between the training of D and G. Training D more frequently than G is actually desired.\n",
        "\n",
        "In this exercise we will follow the indications int the original paper."
      ]
    },
    {
      "metadata": {
        "id": "14vb70Ys_sxa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Fashion cond-dc-GAN\n",
        "\n",
        "- ~~mnist gan~~\n",
        "- ~~fashion gan~~\n",
        "- ~~fashion (32, 32, 1) gan~~\n",
        "- ~~fashion dc-gan~~\n",
        "- ~~fashion cond-dc-gan~~\n",
        "- ~~interpolation fashion cond-dc-gan~~\n",
        "- fashion cond-w-dc-gan\n",
        "- celebA cond-w-dc-gan\n",
        "- interpolation celebA cond-w-dc-gan"
      ]
    },
    {
      "metadata": {
        "id": "dyNPRk36Rzuy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Things to change in WGAN\n",
        "\n",
        "+ Activation of D\n",
        "+ Loss function\n",
        "+ Clipping weights\n",
        "+ Training procedure: now D is trained more often"
      ]
    },
    {
      "metadata": {
        "id": "KlRx5ymdZG0E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "tHZUZmOUNEoJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, BatchNormalization, Reshape, Flatten\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "\n",
        "from keras.layers import Conv2D, UpSampling2D, concatenate, Lambda\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from PIL import Image\n",
        "from matplotlib import animation, rc\n",
        "from IPython.display import Image as ipyImage\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uvvPV_WIZoR2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Function to build the generator\n",
        "\n",
        "The Generator does not change for our WGAN. The generator that we have previously coded stays the same."
      ]
    },
    {
      "metadata": {
        "id": "6gqJ4ZrmNHve",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_generator(noise_size, img_shape, num_classes):\n",
        "  \n",
        "  filters = 512\n",
        "  k_size = 5, 5\n",
        "  k_init = RandomNormal(0, 0.007)\n",
        "  \n",
        "  noise = Input((noise_size,))\n",
        "  labels = Input((num_classes,))\n",
        "  \n",
        "  model_input = concatenate([noise, labels])\n",
        "  \n",
        "  x = Dense(4*4*filters, kernel_initializer=k_init, activation='relu')(model_input)\n",
        "  x = Reshape((4, 4, filters))(x)  # 4, 4\n",
        "  x = BatchNormalization()(x)\n",
        "  x = UpSampling2D()(x)  # 8, 8\n",
        "  \n",
        "  x = Conv2D(filters // 2, k_size, padding='same', kernel_initializer=k_init, activation='relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = UpSampling2D()(x)  # 16, 16\n",
        "  \n",
        "  x = Conv2D(filters // 4, k_size, padding='same', kernel_initializer=k_init, activation='relu')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = UpSampling2D()(x)  # 32, 32\n",
        "  \n",
        "  img = Conv2D(img_shape[-1], k_size, padding='same', kernel_initializer=k_init, activation='tanh')(x)  # 32, 32, 1\n",
        "  \n",
        "  generator = Model([noise, labels], img)\n",
        "  return generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ooEwRbP8eLgw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Function to build the discriminator\n",
        "\n",
        "The Discriminator does change. We have to get rid of the `sigmoid` activation of the last layer of the Discriminator."
      ]
    },
    {
      "metadata": {
        "id": "UysJTIlPOlZS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_discriminator(img_shape, num_classes):\n",
        "  \n",
        "  filters = 512\n",
        "  k_size = 5, 5\n",
        "  k_init = RandomNormal(0, 0.007)\n",
        "  \n",
        "  img = Input(img_shape)  # 32, 32, 1\n",
        "  labels = Input((num_classes,))  # ?, 10\n",
        "  \n",
        "  n_labels = Reshape([1, 1, num_classes])(labels)  # ?, 1, 1, 10\n",
        "  n_labels = Lambda(lambda x: K.tile(x, [1, img_shape[0], img_shape[1], 1]))(n_labels)  # ?, 32, 32, 10\n",
        "  \n",
        "  model_input = concatenate([img, n_labels])  # ?, 32, 32, 11\n",
        "  \n",
        "  x = Conv2D(filters // 4, k_size, strides=(2, 2), padding='same', kernel_initializer=k_init)(model_input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)  # 16, 16\n",
        "  \n",
        "  x = Conv2D(filters // 2, k_size, strides=(2, 2), padding='same', kernel_initializer=k_init)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)  # 8, 8\n",
        "  \n",
        "  x = Conv2D(filters, k_size, strides=(2, 2), padding='same', kernel_initializer=k_init)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(alpha=0.2)(x)  # 4, 4\n",
        "  \n",
        "  x = Flatten()(x)\n",
        "  validity = Dense(1, kernel_initializer=k_init, activation='linear')(x) #CHANGE\n",
        "  \n",
        "  discriminator = Model([img, labels], validity)\n",
        "  return discriminator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uxDM6ldueOte",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Function to compile the models\n",
        "\n",
        "Another change must be done here. We do not want `binary_crossentropy` when we compile the Discriminator and the Combined model anymore. \n",
        "\n",
        "So we're going to implement our own loss function called `critic_loss`, you can use the following template.\n",
        "\n",
        "```python\n",
        "    def critic_loss(y_pred, y_true):\n",
        "      \"\"\"You must implement the value of loss\n",
        "          An expression of one line is just enough\"\"\"\n",
        "      return loss\n",
        "```\n",
        " **Mind that changes in this loss function are going to need changes in how we provide the targets**\n",
        " \n",
        "The Critic loss must output a very positive number if the samples are real, and very negative if the samples are fake. Also, remember that `y_pred` is a vector of values, so we are going to average over them to return a single value for the whole batch. Try to follow me now because this is complicated, here are the rules we need to follow for the case with **real** samples and with **fake** samples:\n",
        "\n",
        "+ In the case that the samples are **real** we expect `y_pred` to be positive, so we can define the loss as the inverse of `y_pred`: if `y_pred` is positive (that's good) then the loss is negative (that's good, we're encouraging this behaviour); if `y_pred` is negative (that's bad) the loss gets positive (we're discouraging this behaviour).\n",
        "\n",
        "  ```python\n",
        "  real_loss = Average(-1 * y_pred)\n",
        "  ```\n",
        "+ A similar reasoning can be followed for the fake case. When samples are **fake** we expect `y_pred` to be negative. So we can use `y_pred` as the loss itself: if `y_pred` is positive (that's bad) then the loss is also positive (we're discouraging this behaviour); if `y_pred` is negative (that's good) then the loss is also negative (we're encouraging this behaviour).\n",
        "\n",
        "  ```python\n",
        "  fake_loss = Average(+1 * y_pred)\n",
        "  ```\n",
        "\n",
        "An easy way of putting both rules together is to set the targets `y_true` as `-1` (for the **real** case) and `+1` (for the **fake** case), and using the `K.mean()` function of keras. Remember to import the keras backend\n",
        "\n",
        "```python\n",
        "import keras.backend as K\n",
        "K.mean(something_to_average)\n",
        "```\n",
        "\n",
        "When you finish defining your loss function, add it as a compile option for the Discriminator (or Critic) and the Combined model. Also, computing the accuracy of the Discriminator (`metrics=['accuracy']`) makes  not much sense in this case so it should be removed."
      ]
    },
    {
      "metadata": {
        "id": "3LR6lPzpZVVb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def critic_loss(y_pred, y_true):\n",
        "  return K.mean(y_pred * y_true) #CHANGE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gEfTuy93PRNb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_compiled_models(generator, discriminator, noise_size, num_classes):\n",
        "  \n",
        "  optimizer = RMSprop(0.00005)\n",
        "  \n",
        "  discriminator.compile(optimizer, loss=critic_loss) #CHANGE\n",
        "  discriminator.trainable = False\n",
        "  \n",
        "  noise = Input((noise_size,))\n",
        "  labels = Input((num_classes,))\n",
        "  \n",
        "  img = generator([noise, labels])\n",
        "  validity = discriminator([img, labels])\n",
        "  combined = Model([noise, labels], validity)\n",
        "  \n",
        "  combined.compile(optimizer, loss=critic_loss) #CHANGE\n",
        "  \n",
        "  return generator, discriminator, combined"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ocu71YfmeSIH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Function to sample and save generated images"
      ]
    },
    {
      "metadata": {
        "id": "X4-ejg8UTwqV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample_imgs(generator, noise_size, gen_loss_memory, step, plot_img=True, cond=False, num_classes=10):\n",
        "  np.random.seed(0)\n",
        "  \n",
        "  r, c = num_classes, 10\n",
        "  if cond:\n",
        "    noise = np.random.normal(0, 1, (c, noise_size))\n",
        "    noise = np.tile(noise, (r, 1))\n",
        "\n",
        "    sampled_labels = np.arange(r).reshape(-1, 1)\n",
        "    sampled_labels = to_categorical(sampled_labels, r)\n",
        "    sampled_labels = np.repeat(sampled_labels, c, axis=0)\n",
        "\n",
        "    imgs = generator.predict([noise, sampled_labels])\n",
        "  else:\n",
        "    noise = np.random.normal(0, 1, (r*c, noise_size))\n",
        "    imgs = generator.predict_on_batch(noise)\n",
        "  \n",
        "  imgs = imgs / 2 + 0.5\n",
        "  imgs = np.reshape(imgs, [r, c, imgs.shape[1], imgs.shape[2], -1])\n",
        "  \n",
        "  gs = gridspec.GridSpec(r, 2*c)\n",
        "  \n",
        "  figsize = 2 * c, 1 * r\n",
        "  fig = plt.figure(figsize=figsize)\n",
        "  \n",
        "  for i in range(r):\n",
        "    for j in range(c):\n",
        "      img = imgs[i, j] if len(imgs.shape) == 4 else imgs[i, j, :, :, 0]\n",
        "      plt.subplot(gs[i, j])\n",
        "      plt.imshow(img, cmap='gray')\n",
        "      plt.axis('off')\n",
        "  plt.subplot(gs[:, c:])\n",
        "  plt.plot(gen_loss_memory)\n",
        "  plt.ylim(-0.05, 0.5)\n",
        "  plt.gca().tick_params(axis='y', direction='in', pad=-20)\n",
        "  #plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.savefig(f'/content/images/{step}.png')\n",
        "  if plot_img:\n",
        "    plt.show()\n",
        "  plt.close()\n",
        "  \n",
        "  np.random.seed(None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K3ofIxpleVoc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Function to train the models\n",
        "\n",
        "This function is the one that requires more change.\n",
        "* __Loss function__, despite what we did on the code above, now we need to change the targets `y_true` (or `real_validity` and `gen_validity` in the code below).\n",
        "\n",
        "+ __Clipping weights and optimiser__. We need to guarantee that the function that D computes is K-Lipschitz. One way to ensure this is to clip the weights of D to a small value.\n",
        "\n",
        "  After processing a full batch of real and fake samples, and after actualising the weights accordingly, we need to iterate over the layers and to clip the weights to a small constant **c** (0.01) in the original paper.\n",
        "  \n",
        "+ __Training procedure: now D is trained more often__, in the WGAN it is not important to keep a balance between the training of D and G. Training D more frequently than G is actually desired.\n",
        "\n",
        "  We can use a for loop to train D more."
      ]
    },
    {
      "metadata": {
        "id": "wHI7xwGMQRAW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(models, noise_size, img_shape, num_classes, batch_size, steps, n_critic, c): #CHANGE, add n_critic and c\n",
        "  \n",
        "  generator, discriminator, combined = models\n",
        "  #get real data\n",
        "  (X_train, Y_train), (X_val, Y_val) = fashion_mnist.load_data()\n",
        "  fashion_mnist_imgs = np.concatenate((X_train, X_val)) / 127.5 - 1  # 100.000, 28, 28\n",
        "  fashion_mnist_imgs = np.pad(fashion_mnist_imgs, ((0, 0), (2, 2), (2, 2)), 'constant', constant_values=-1)  # 100.000, 32, 32\n",
        "  fashion_mnist_imgs = np.expand_dims(fashion_mnist_imgs, axis=-1)  # 100.000, 32, 32, 1\n",
        "  fashion_mnist_labels = np.concatenate((Y_train, Y_val))\n",
        "  \n",
        "  gen_loss_memory = [] # to save gen_loss during training\n",
        "  \n",
        "  for step in range(1, steps + 1):\n",
        "    # train discriminator\n",
        "    for _ in range(n_critic): #CHANGE, add loop\n",
        "      inds = np.random.randint(0, fashion_mnist_imgs.shape[0], batch_size)\n",
        "\n",
        "      labels = fashion_mnist_labels[inds]\n",
        "      labels = to_categorical(labels, num_classes)\n",
        "\n",
        "      real_imgs = fashion_mnist_imgs[inds]\n",
        "      real_validity = -np.ones(batch_size) #CHANGE\n",
        "\n",
        "      noise = np.random.normal(0, 1, (batch_size, noise_size))\n",
        "      gen_imgs = generator.predict([noise, labels])\n",
        "      gen_validity = np.ones(batch_size) #CHANGE\n",
        "\n",
        "      r_loss = discriminator.train_on_batch([real_imgs, labels], real_validity)\n",
        "      g_loss = discriminator.train_on_batch([gen_imgs, labels], gen_validity)\n",
        "      disc_loss = np.add(r_loss, g_loss) / 2\n",
        "      gen_loss_memory.append(g_loss)\n",
        "      \n",
        "      #CHANGE, clip weights\n",
        "      for layer in discriminator.layers:\n",
        "        weights = layer.get_weights()\n",
        "        clipped_weights = [np.clip(w, -c, c) for w in weights]\n",
        "        layer.set_weights(clipped_weights)\n",
        "    \n",
        "    # train generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, noise_size))\n",
        "    gen_validity = -np.ones(batch_size)\n",
        "    gen_loss = combined.train_on_batch([noise, labels], gen_validity)\n",
        "    \n",
        "    #print progress\n",
        "    if step % 50 == 0:\n",
        "      print('step: %d, D_loss: %f G_loss: %f' % (step, disc_loss, gen_loss))\n",
        "    \n",
        "    # save_samples\n",
        "    if step % 50 == 0:\n",
        "      sample_imgs(generator, noise_size, gen_loss_memory, step, cond=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jHnx_qUceZyc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "1-ePrmDUVBYM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%rm -r /content/images\n",
        "%mkdir /content/images\n",
        "noise_size = 100\n",
        "img_shape = 32, 32, 1\n",
        "num_classes = 10\n",
        "batch_size = 64\n",
        "steps = 5000\n",
        "\n",
        "#CHANGE, add WGAN specific parameters\n",
        "n_critic = 5\n",
        "c = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lhkkM5ctecbb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate the models"
      ]
    },
    {
      "metadata": {
        "id": "FCr5cDHOZCdW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator = build_generator(noise_size, img_shape, num_classes)\n",
        "discriminator = build_discriminator(img_shape, num_classes)\n",
        "compiled_models = get_compiled_models(generator, discriminator, noise_size, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zTkuRejSefba",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the models"
      ]
    },
    {
      "metadata": {
        "id": "JznD-Lt6Y2h_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train(compiled_models, noise_size, img_shape, num_classes, batch_size, steps, n_critic, c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7BDEKYo9ehjq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Display samples\n",
        "\n",
        "Let's start by checking the images that we have stored."
      ]
    },
    {
      "metadata": {
        "id": "TAkraVLWiohI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%ls /content/images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KfNsB1NO2tQM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can check any image you wish by doing:"
      ]
    },
    {
      "metadata": {
        "id": "Z0JNe-A02wVK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_number = 50\n",
        "ipyImage('/content/images/%d.png' % image_number)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4eGqyamNc69P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Do an animation\n",
        "\n",
        "Probably the best way of showing the training process is by doing an animation with all the images. The next cell will do it for you."
      ]
    },
    {
      "metadata": {
        "id": "Pi2ex7lvh335",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class AnimObject(object):\n",
        "    def __init__(self, images):\n",
        "        print(len(images))\n",
        "        self.fig, self.ax = plt.subplots()\n",
        "        self.ax.set_title(\"\")\n",
        "        self.fig.set_size_inches((20, 10))\n",
        "        self.plot = plt.imshow(images[0])\n",
        "        plt.tight_layout()\n",
        "        self.images = images\n",
        "        \n",
        "    def init(self):\n",
        "        self.plot.set_data(self.images[0])\n",
        "        self.ax.grid(False)\n",
        "        return (self.plot,)\n",
        "      \n",
        "    def animate(self, i):\n",
        "        self.plot.set_data(self.images[i])\n",
        "        self.ax.grid(False)\n",
        "        self.ax.set_xticks([])\n",
        "        self.ax.set_yticks([])\n",
        "        self.ax.set_title(\"index {}\".format(i))\n",
        "        return (self.plot,)\n",
        "\n",
        "def get_figures(template, indices):\n",
        "    import os.path\n",
        "    images = []\n",
        "    for index in indices:\n",
        "        if os.path.isfile(template.format(index)):\n",
        "            images.append(Image.open(template.format(index)))\n",
        "    return images\n",
        "\n",
        "\n",
        "images = get_figures(\"/content/images/{}.png\", \n",
        "                     range(0, 50 * len(listdir('/content/images')) + 1, 50))\n",
        "print(images)\n",
        "animobject = AnimObject(images)\n",
        "anim = animation.FuncAnimation(\n",
        "              animobject.fig,\n",
        "              animobject.animate,\n",
        "              frames=len(animobject.images),\n",
        "              interval=150,\n",
        "              blit=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J3vKJHaVkdwT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OUlOsOvEiGSg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What do we make of the WGAN?\n",
        "\n",
        "Here are some takes on the WGAN compared to the original GAN formulation.\n",
        "\n",
        "+ **It trains slower**. When using the Wasserstein distance to train the Generator, it is interesting to have a strong Discriminator. To do that we need to train the Discriminator more often and so we need more time to train the WGAN.\n",
        "\n",
        "The following two points are well represented in this example, presumably because the fashion MNIST is a trivial (very easy) dataset.\n",
        "\n",
        "+ **It is more stable**. Sometimes GANs can _unlearn_ in just a few iterations what took them several minutes to learn. WGAN tends to be more stable.\n",
        "\n",
        "+ **Error indicator** that correlates with the quality of the images. \n",
        "\n",
        "Also, and because the Discriminator acts like a critic and not like a classifier, sometimes the generator error can be negative. This is the equivalent of the Discriminator considering generated samples as real instead of fake."
      ]
    }
  ]
}